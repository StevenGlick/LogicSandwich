#!/usr/bin/env python3
"""
INTEGRATION TEST: Water Domain
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Runs the full pipeline on a fresh topic (water) to exercise:
  - Pulse engine (merge, transitive, cross-domain, analogy)
  - Domain scope (should block nonsense but allow real connections)
  - Bridge detection (should find bridges across many types)
  - Optimizer (hierarchy, centrality, FTS, domains)
  - Query engine (graph walk, path finding)

No LLM needed ‚Äî tests the CPU-side operations.
"""

import sqlite3
import json
import os
import sys
import time
from collections import defaultdict

# ‚îÄ‚îÄ‚îÄ Setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

TEST_DB = "test_water.db"

# Clean start
if os.path.exists(TEST_DB):
    os.remove(TEST_DB)

print(f"{'‚ïê'*60}")
print(f"  üß™ INTEGRATION TEST: Water Domain")
print(f"{'‚ïê'*60}")

# ‚îÄ‚îÄ‚îÄ Create Database ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

conn = sqlite3.connect(TEST_DB)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

cur.executescript("""
    CREATE TABLE IF NOT EXISTS entries (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        subject TEXT NOT NULL,
        predicate TEXT NOT NULL,
        object TEXT NOT NULL,
        truth_value TEXT DEFAULT 'T',
        evidence_for TEXT DEFAULT '[]',
        evidence_against TEXT DEFAULT '[]',
        source TEXT DEFAULT 'seed',
        generation INTEGER DEFAULT 0,
        grade_level INTEGER DEFAULT 0,
        needs_verification INTEGER DEFAULT 0,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    CREATE INDEX IF NOT EXISTS idx_subj ON entries(subject);
    CREATE INDEX IF NOT EXISTS idx_pred ON entries(predicate);
    CREATE INDEX IF NOT EXISTS idx_obj ON entries(object);
    CREATE INDEX IF NOT EXISTS idx_tv ON entries(truth_value);
""")
conn.commit()

# ‚îÄ‚îÄ‚îÄ Load Seed Data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  ‚îÄ‚îÄ Phase 1: Seed Data ‚îÄ‚îÄ")
with open("water_seed.json") as f:
    seeds = json.load(f)

for s in seeds:
    cur.execute("""
        INSERT INTO entries (subject, predicate, object, truth_value, source, generation)
        VALUES (?, ?, ?, 'T', 'seed', 0)
    """, (s["subject"], s["predicate"], s["object"]))

conn.commit()
cur.execute("SELECT COUNT(*) FROM entries")
seed_count = cur.fetchone()[0]
print(f"  Loaded {seed_count} seed entries about water")

# ‚îÄ‚îÄ‚îÄ Domain Scope Check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  ‚îÄ‚îÄ Phase 2: Domain Scope Analysis ‚îÄ‚îÄ")
from domain_scope import DomainClassifier, TypeGate, NonsenseFilter

classifier = DomainClassifier(conn)

# Classify all subjects
cur.execute("SELECT DISTINCT subject FROM entries")
subjects = [r[0] for r in cur.fetchall()]
domain_map = defaultdict(list)
unclassified = []

for s in subjects:
    domains = classifier.classify(s)
    if domains:
        for d in domains:
            domain_map[d].append(s)
    else:
        unclassified.append(s)

print(f"  Subjects: {len(subjects)} total")
print(f"  Classified into domains:")
for d in sorted(domain_map, key=lambda x: -len(domain_map[x])):
    print(f"    {d:20s} {len(domain_map[d]):3d} subjects")
print(f"  Unclassified: {len(unclassified)}")
if unclassified[:10]:
    for u in unclassified[:10]:
        print(f"    ? {u}")

# ‚îÄ‚îÄ‚îÄ Pulse Engine (CPU Operations) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  ‚îÄ‚îÄ Phase 3: Pulse Engine ‚îÄ‚îÄ")

def get_all_entries():
    cur.execute("SELECT * FROM entries WHERE truth_value='T'")
    return [dict(r) for r in cur.fetchall()]

def add_entry(subj, pred, obj, source, gen, tv="T"):
    """Add with dedup check."""
    cur.execute("SELECT id FROM entries WHERE subject=? AND predicate=? AND object=?",
                (subj, pred, obj))
    if cur.fetchone():
        return None
    cur.execute("""
        INSERT INTO entries (subject, predicate, object, truth_value, source, generation)
        VALUES (?, ?, ?, ?, ?, ?)
    """, (subj, pred, obj, tv, source, gen))
    conn.commit()
    return cur.lastrowid

# --- Merge ---
print(f"\n  Merge (combining same-subject entries)...")
merge_count = 0
entries = get_all_entries()
subject_entries = defaultdict(list)
for e in entries:
    if "," not in e["subject"]:
        subject_entries[e["subject"]].append(e)

for subj, ents in subject_entries.items():
    if len(ents) >= 3:
        preds = set(e["predicate"] for e in ents)
        objs = set(e["object"] for e in ents)
        summary = f"has_{len(preds)}_predicates_and_{len(objs)}_objects"
        r = add_entry(subj, "profile_size", summary, "pulse:merge", 1)
        if r:
            merge_count += 1

print(f"    Generated: {merge_count} merge entries")

# --- Transitive ---
print(f"\n  Transitive (if A‚ÜíB and B‚ÜíC then A‚ÜíC)...")
trans_count = 0
trans_blocked = 0
trans_blocked_reasons = defaultdict(int)
entries = get_all_entries()

# Build lookup
subj_lookup = defaultdict(list)
for e in entries:
    subj_lookup[e["subject"]].append(e)

gate = TypeGate(conn)
nsf = NonsenseFilter(classifier)

from predicate_rules import TransitiveFilter
tf = TransitiveFilter()

for e in entries:
    # Find entries where this entry's object is someone else's subject
    obj = e["object"]
    if obj in subj_lookup:
        for e2 in subj_lookup[obj]:
            # Chain: e.subject --[e.predicate]--> obj --[e2.predicate]--> e2.object
            new_subj = e["subject"]
            link_pred = e["predicate"]      # how A connects to B
            new_pred = e2["predicate"]       # B's property we'd inherit
            new_obj = e2["object"]
            
            # Skip boring chains
            if new_pred in ("profile_size",):
                continue
            if new_subj == new_obj:
                continue
            
            # Predicate transparency check (the new fix!)
            allowed_tf, reason_tf = tf.check_chain(link_pred, new_pred)
            if not allowed_tf:
                trans_blocked += 1
                trans_blocked_reasons[reason_tf] += 1
                continue
            
            # Scope check
            allowed_ns, reason_ns = nsf.check(new_subj, new_pred, new_obj)
            if not allowed_ns:
                trans_blocked += 1
                trans_blocked_reasons[reason_ns] += 1
                continue
            
            allowed_tg, reason_tg, conf = gate.check_transitive(new_subj, new_pred, new_obj)
            if not allowed_tg:
                trans_blocked += 1
                trans_blocked_reasons[reason_tg] += 1
                continue
            
            # Low confidence ‚Üí Maybe
            tv = "M" if conf < 0.5 else "T"
            r = add_entry(new_subj, new_pred, new_obj, "pulse:transitive", 1, tv)
            if r:
                trans_count += 1

print(f"    Generated: {trans_count} transitive entries")
print(f"    Blocked: {trans_blocked} (predicate transparency + domain scope)")
if trans_blocked_reasons:
    print(f"    Block reasons:")
    for reason, count in sorted(trans_blocked_reasons.items(), key=lambda x: -x[1])[:10]:
        print(f"      {count:4d}√ó {reason[:70]}")

# --- Cross-Domain ---
print(f"\n  Cross-Domain (different subjects sharing properties)...")
cross_count = 0
cross_blocked = 0
entries = get_all_entries()

# Group by (predicate, object)
po_groups = defaultdict(list)
for e in entries:
    if "," not in e["subject"]:
        po_groups[(e["predicate"], e["object"])].append(e["subject"])

for (pred, obj), subs in po_groups.items():
    if len(subs) < 2 or len(subs) > 8:
        continue
    # Skip trivial predicates
    if pred in ("profile_size",):
        continue
    
    for i, s1 in enumerate(subs):
        for s2 in subs[i+1:]:
            if s1 == s2:
                continue
            
            # Check domain compatibility
            compat = classifier.compatibility_score(s1, s2)
            
            compound = f"{s1},{s2}"
            new_pred = f"both_{pred}"
            
            if compat < 0.3 and compat > 0.0:
                cross_blocked += 1
                continue
            
            r = add_entry(compound, new_pred, obj, "pulse:cross_domain", 1)
            if r:
                cross_count += 1

print(f"    Generated: {cross_count} cross-domain entries")
print(f"    Blocked: {cross_blocked} (low compatibility)")

# --- Analogy ---
print(f"\n  Analogy (subjects with similar predicate profiles)...")
analogy_count = 0
entries = get_all_entries()

# Build predicate profile per subject
profiles = defaultdict(set)
for e in entries:
    if "," not in e["subject"]:
        profiles[e["subject"]].add(e["predicate"])

# Compare profiles
subjects_list = [s for s in profiles if len(profiles[s]) >= 3]
for i, s1 in enumerate(subjects_list):
    for s2 in subjects_list[i+1:]:
        shared = profiles[s1] & profiles[s2]
        total = profiles[s1] | profiles[s2]
        if len(total) == 0:
            continue
        similarity = len(shared) / len(total)  # Jaccard
        
        if similarity >= 0.3 and len(shared) >= 2:
            # Check domain gate
            allowed, reason = gate.check_analogy(s1, s2)
            if not allowed:
                continue
            
            shared_str = ",".join(sorted(shared)[:5])
            r = add_entry(f"{s1},{s2}", "structurally_similar",
                         f"shared_predicates:{shared_str}",
                         "pulse:analogy", 1)
            if r:
                analogy_count += 1

print(f"    Generated: {analogy_count} analogy entries")

# ‚îÄ‚îÄ‚îÄ Post-Pulse Stats ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

cur.execute("SELECT COUNT(*) FROM entries")
total = cur.fetchone()[0]
cur.execute("SELECT source, COUNT(*) as cnt FROM entries GROUP BY source ORDER BY cnt DESC")
sources = cur.fetchall()

print(f"\n  ‚îÄ‚îÄ Pulse Results ‚îÄ‚îÄ")
print(f"  Total entries: {seed_count} seed ‚Üí {total} total (+{total-seed_count})")
print(f"  By source:")
for s in sources:
    bar = "‚ñà" * min(s[1]//2, 30)
    print(f"    {s[0]:25s} {s[1]:4d} {bar}")

# ‚îÄ‚îÄ‚îÄ Optimizer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  ‚îÄ‚îÄ Phase 4: Optimization ‚îÄ‚îÄ")

# Import and run optimizer functions
from optimize import (normalize_predicates, build_hierarchy, 
                      compute_centrality, build_fts_index, 
                      build_synonyms, cluster_domains, deduplicate)

print(f"\n  Predicate normalization...")
normalize_predicates(conn)

print(f"\n  Deduplication...")
deduplicate(conn)

print(f"\n  Hierarchy index...")
build_hierarchy(conn)

print(f"\n  Centrality scoring...")
compute_centrality(conn)

print(f"\n  Full-text search...")
build_fts_index(conn)

print(f"\n  Synonym expansion...")
build_synonyms(conn)

print(f"\n  Domain clustering...")
cluster_domains(conn)

# ‚îÄ‚îÄ‚îÄ Bridge Detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  ‚îÄ‚îÄ Phase 5: Bridge Detection ‚îÄ‚îÄ")

from bridges import BridgeSystem, BridgeDiscovery, SEED_BRIDGES

# Load seed bridges into test DB
bs = BridgeSystem(TEST_DB)
loaded = 0
for bridge in SEED_BRIDGES:
    r = bs.add_bridge(**bridge)
    if r:
        loaded += 1
print(f"  Loaded {loaded} seed bridges")

# Now check: which seed bridges are RELEVANT to our water knowledge?
print(f"\n  Bridges relevant to water domain:")
water_terms = set()
cur.execute("SELECT DISTINCT subject FROM entries WHERE subject NOT LIKE '%,%'")
water_terms.update(r[0] for r in cur.fetchall())
cur.execute("SELECT DISTINCT object FROM entries WHERE object NOT LIKE '%,%'")
water_terms.update(r[0] for r in cur.fetchall())

relevant_bridges = []
all_bridges = bs.all_bridges()
for b in all_bridges:
    # Check if either term appears in our water knowledge
    ta = b["term_a"]
    tb = b["term_b"]
    
    relevant = False
    # Direct match
    if ta in water_terms or tb in water_terms:
        relevant = True
    # Substring match (e.g., "wave" in "ocean_wave")
    for wt in water_terms:
        if ta in wt or tb in wt or wt in ta or wt in tb:
            relevant = True
            break
    
    if relevant:
        relevant_bridges.append(b)

type_icons = {
    "historical": "üìú", "inspirational": "üí°", "structural": "üî∑",
    "metaphorical": "üé≠", "emergent": "‚ú®", "causal": "‚ö°",
    "cultural": "üåç", "etymological": "üìñ", "scale": "üî¨",
    "pedagogical": "üéì", "contradictory": "‚öîÔ∏è", "experiential": "üñêÔ∏è",
    "spiritual": "üïâÔ∏è", "religious": "‚õ™", "ethical": "‚öñÔ∏è",
    "aesthetic": "üé®", "rumor": "üí≠", "mythological": "üêâ",
    "linguistic": "üó£Ô∏è", "technological": "üîß", "emotional": "üíô",
    "pattern": "üåÄ", "philosophical": "üèõÔ∏è", "sensory": "üëÅÔ∏è", "humor": "üòÑ",
}

bridge_types_found = defaultdict(list)
for b in relevant_bridges:
    bt = b["bridge_type"]
    bridge_types_found[bt].append(b)

print(f"\n  Found {len(relevant_bridges)} relevant bridges across {len(bridge_types_found)} types:")
for bt in sorted(bridge_types_found, key=lambda x: -len(bridge_types_found[x])):
    icon = type_icons.get(bt, "üîó")
    bridges = bridge_types_found[bt]
    print(f"\n    {icon} {bt} ({len(bridges)}):")
    for b in bridges:
        print(f"      {b['term_a']} ‚Üî {b['term_b']}")
        print(f"        {b['reason'][:100]}...")

# ‚îÄ‚îÄ‚îÄ Bridge Discovery from Knowledge ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  ‚îÄ‚îÄ Phase 6: Auto-Discovered Bridges ‚îÄ‚îÄ")

# Find cross-domain entries that COULD be bridges
cur.execute("""
    SELECT * FROM entries 
    WHERE subject LIKE '%,%' AND source='pulse:cross_domain'
    ORDER BY subject
""")
cross_entries = [dict(r) for r in cur.fetchall()]

print(f"\n  Cross-domain patterns that could be bridges ({len(cross_entries)}):")

interesting = []
for e in cross_entries:
    parts = e["subject"].split(",")
    if len(parts) == 2:
        s1, s2 = parts
        d1 = classifier.classify(s1)
        d2 = classifier.classify(s2)
        
        # Flag as interesting if they span different domains
        if d1 and d2 and d1 != d2:
            interesting.append({
                "term_a": s1, "term_b": s2,
                "domain_a": str(d1), "domain_b": str(d2),
                "shared": f"{e['predicate']}({e['object']})",
            })

if interesting:
    for item in interesting[:15]:
        print(f"    ‚ú® {item['term_a']} ‚Üî {item['term_b']}")
        print(f"       Shared: {item['shared']}")
        print(f"       Domains: {item['domain_a']} ‚Üî {item['domain_b']}")
else:
    print(f"    (All cross-domain patterns were within same domain cluster)")

# Show same-domain patterns too (still valuable)
print(f"\n  Same-domain cross-patterns (first 15):")
for e in cross_entries[:15]:
    parts = e["subject"].split(",")
    if len(parts) == 2:
        print(f"    {parts[0]:25s} ‚Üî {parts[1]:25s} | {e['predicate']}({e['object'][:30]})")

# ‚îÄ‚îÄ‚îÄ Query Test ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  ‚îÄ‚îÄ Phase 7: Query Tests ‚îÄ‚îÄ")

def simple_query(term):
    """Search for everything about a term."""
    results = []
    cur.execute("SELECT * FROM entries WHERE subject=? AND truth_value='T'", (term,))
    results.extend(dict(r) for r in cur.fetchall())
    cur.execute("SELECT * FROM entries WHERE object=? AND truth_value='T'", (term,))
    results.extend(dict(r) for r in cur.fetchall())
    return results

def graph_walk(start, depth=2):
    """BFS walk from a starting node."""
    visited = set()
    queue = [(start, 0)]
    nodes = set()
    edges = []
    
    while queue:
        current, d = queue.pop(0)
        if current in visited or d > depth:
            continue
        visited.add(current)
        nodes.add(current)
        
        cur.execute("SELECT predicate, object FROM entries WHERE subject=? AND truth_value='T' AND subject NOT LIKE '%,%'", (current,))
        for r in cur.fetchall():
            obj = r[1]
            edges.append((current, r[0], obj))
            if d < depth and "," not in obj:
                queue.append((obj, d+1))
    
    return nodes, edges

# Test 1: Direct lookup
print(f"\n  Query: 'water'")
results = simple_query("water")
print(f"    {len(results)} facts found")
for r in results[:8]:
    print(f"      {r['subject']} ‚Üí {r['predicate']}({r['object'][:40]})")
if len(results) > 8:
    print(f"      ... +{len(results)-8} more")

# Test 2: Graph walk
print(f"\n  Graph walk from 'water' (depth 2):")
nodes, edges = graph_walk("water", 2)
print(f"    {len(nodes)} nodes, {len(edges)} edges")
print(f"    Nodes: {', '.join(sorted(list(nodes)[:15]))}")
if len(nodes) > 15:
    print(f"           ... +{len(nodes)-15} more")

# Test 3: Graph walk from wave
print(f"\n  Graph walk from 'wave' (depth 2):")
nodes2, edges2 = graph_walk("wave", 2)
print(f"    {len(nodes2)} nodes, {len(edges2)} edges")
print(f"    Nodes: {', '.join(sorted(list(nodes2)[:15]))}")

# Test 4: Path finding
print(f"\n  Path: 'water' ‚Üí 'egyptian_civilization'")
# BFS path finding
def find_path(start, end, max_depth=5):
    visited = set()
    queue = [(start, [(start, None, None)])]
    
    while queue:
        current, path = queue.pop(0)
        if current == end:
            return path
        if current in visited or len(path) > max_depth:
            continue
        visited.add(current)
        
        cur.execute("SELECT predicate, object FROM entries WHERE subject=? AND truth_value='T' AND subject NOT LIKE '%,%'", (current,))
        for r in cur.fetchall():
            if "," not in r[1]:
                queue.append((r[1], path + [(r[1], r[0], current)]))
    
    return None

path = find_path("water", "egyptian_civilization", 6)
if path:
    print(f"    Found! {len(path)-1} hops:")
    for i, (node, pred, via) in enumerate(path):
        if pred:
            print(f"      {'‚Üí':>4s} {via} ‚Üí {pred} ‚Üí {node}")
        else:
            print(f"      START {node}")
else:
    print(f"    No path found (trying via nile...)")
    # Manual trace
    path1 = find_path("water", "nile", 4)
    path2 = find_path("nile", "egyptian_civilization", 4)
    if path1:
        print(f"    water ‚Üí nile: {len(path1)-1} hops")
    if path2:
        print(f"    nile ‚Üí egyptian_civilization: {len(path2)-1} hops")

# Test 5: Cross-bridge query
print(f"\n  Bridge-aware query: 'What connects water to emotions?'")
# Find all emotion-related entries about water
emotion_entries = []
cur.execute("""
    SELECT * FROM entries WHERE truth_value='T' AND (
        object LIKE '%emotion%' OR object LIKE '%sadness%' OR 
        object LIKE '%joy%' OR object LIKE '%grief%' OR
        object LIKE '%overwhelm%' OR object LIKE '%calming%' OR
        predicate LIKE '%feel%' OR predicate LIKE '%associated%'
    )
""")
emotion_entries = [dict(r) for r in cur.fetchall()]
print(f"    Direct emotional connections: {len(emotion_entries)}")
for e in emotion_entries:
    print(f"      {e['subject']} ‚Üí {e['predicate']}({e['object']})")

# Check bridges for emotional connections
emotion_bridges = [b for b in relevant_bridges if b["bridge_type"] in ("emotional", "sensory", "experiential")]
if emotion_bridges:
    print(f"    Relevant bridges: {len(emotion_bridges)}")
    for b in emotion_bridges:
        icon = type_icons.get(b["bridge_type"], "üîó")
        print(f"      {icon} {b['term_a']} ‚Üî {b['term_b']} ({b['bridge_type']})")

# ‚îÄ‚îÄ‚îÄ Final Report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

print(f"\n  {'‚ïê'*60}")
print(f"  üìä FINAL REPORT")
print(f"  {'‚ïê'*60}")

cur.execute("SELECT COUNT(*) FROM entries")
final_total = cur.fetchone()[0]
cur.execute("SELECT COUNT(*) FROM entries WHERE truth_value='T'")
true_count = cur.fetchone()[0]
cur.execute("SELECT COUNT(*) FROM entries WHERE truth_value='M'")
maybe_count = cur.fetchone()[0]

print(f"\n  Knowledge Base:")
print(f"    Seed entries:        {seed_count}")
print(f"    After 1 pulse:       {final_total} (+{final_total - seed_count})")
print(f"    True:                {true_count}")
print(f"    Maybe (need verify): {maybe_count}")

print(f"\n  Pulse Output:")
print(f"    Merge:        {merge_count}")
print(f"    Transitive:   {trans_count} (blocked: {trans_blocked})")
print(f"    Cross-domain: {cross_count} (blocked: {cross_blocked})")
print(f"    Analogy:      {analogy_count}")
print(f"    TOTAL:        {merge_count + trans_count + cross_count + analogy_count}")

print(f"\n  Bridge Coverage:")
print(f"    Relevant bridges:    {len(relevant_bridges)} of {len(all_bridges)}")
print(f"    Bridge types hit:    {len(bridge_types_found)} of {len(type_icons)}")
print(f"    Types covered:       {', '.join(sorted(bridge_types_found.keys()))}")

types_missed = set(type_icons.keys()) - set(bridge_types_found.keys())
if types_missed:
    print(f"    Types not hit:       {', '.join(sorted(types_missed))}")
    print(f"    (These would activate with broader knowledge)")

# Centrality top concepts
try:
    cur.execute("SELECT subject, score FROM centrality ORDER BY score DESC LIMIT 10")
    print(f"\n  Most Connected Concepts:")
    for r in cur.fetchall():
        bar = "‚ñà" * min(int(r[1])//2, 25)
        print(f"    {r[0]:25s} score={r[1]:5.0f} {bar}")
except (sqlite3.OperationalError, TypeError):
    pass

print(f"\n  ‚úì Test complete")

conn.close()
