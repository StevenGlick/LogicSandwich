{
  "topic": "artificial_intelligence_comprehensive",
  "entries": [
    {"id":"001","subject":"neural_network","predicate":"is_a","object":"computational_model","quantifier":"universal","truth_value":"T","evidence_for":["mathematical model inspired by biological neurons"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"002","subject":"neural_network","predicate":"has_component","object":"input_layer","quantifier":"universal","truth_value":"T","evidence_for":["receives input data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"003","subject":"neural_network","predicate":"has_component","object":"hidden_layer","quantifier":"possible","truth_value":"T","evidence_for":["intermediate processing, 0 or more"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"004","subject":"neural_network","predicate":"has_component","object":"output_layer","quantifier":"universal","truth_value":"T","evidence_for":["produces final result"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"005","subject":"neuron","predicate":"performs","object":"weighted_sum","quantifier":"universal","truth_value":"T","evidence_for":["sum of inputs * weights + bias"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"006","subject":"neuron","predicate":"applies","object":"activation_function","quantifier":"universal","truth_value":"T","evidence_for":["introduces nonlinearity"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"007","subject":"activation_function","predicate":"provides","object":"nonlinearity","quantifier":"universal","truth_value":"T","evidence_for":["without it layers collapse to single linear transform"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"008","subject":"relu","predicate":"is_a","object":"activation_function","quantifier":"universal","truth_value":"T","evidence_for":["rectified linear unit, most popular"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"009","subject":"sigmoid","predicate":"is_a","object":"activation_function","quantifier":"universal","truth_value":"T","evidence_for":["maps to 0-1"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"010","subject":"backpropagation","predicate":"is_a","object":"training_algorithm","quantifier":"universal","truth_value":"T","evidence_for":["propagates error backward"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"011","subject":"backpropagation","predicate":"uses","object":"gradient_descent","quantifier":"universal","truth_value":"T","evidence_for":["calculates gradient of loss"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"012","subject":"gradient_descent","predicate":"is_a","object":"optimization_algorithm","quantifier":"universal","truth_value":"T","evidence_for":["iteratively minimizes loss"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"013","subject":"transformer","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["Attention Is All You Need 2017"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"014","subject":"transformer","predicate":"uses","object":"self_attention","quantifier":"universal","truth_value":"T","evidence_for":["core mechanism"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"015","subject":"transformer","predicate":"does_not_use","object":"recurrence","quantifier":"universal","truth_value":"T","evidence_for":["eliminated RNN sequential processing"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"016","subject":"self_attention","predicate":"computes","object":"query_key_value_matrices","quantifier":"universal","truth_value":"T","evidence_for":["Q K V via learned weight matrices"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"017","subject":"self_attention","predicate":"enables","object":"parallel_processing","quantifier":"universal","truth_value":"T","evidence_for":["all tokens simultaneously"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"018","subject":"self_attention","predicate":"has_complexity","object":"quadratic_in_sequence_length","quantifier":"universal","truth_value":"T","evidence_for":["O(n^2) in context window"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"019","subject":"multi_head_attention","predicate":"is_a","object":"self_attention_variant","quantifier":"universal","truth_value":"T","evidence_for":["multiple parallel attention heads"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"020","subject":"multi_head_attention","predicate":"captures","object":"multiple_relationship_types","quantifier":"universal","truth_value":"T","evidence_for":["syntactic, semantic, positional in parallel"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"021","subject":"rnn","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["recurrent neural network"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"022","subject":"rnn","predicate":"uses","object":"recurrence","quantifier":"universal","truth_value":"T","evidence_for":["feedback loops, sequential"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"023","subject":"rnn","predicate":"suffers_from","object":"vanishing_gradient","quantifier":"common","truth_value":"T","evidence_for":["gradients diminish through long sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"024","subject":"rnn","predicate":"processes","object":"sequential_data","quantifier":"universal","truth_value":"T","evidence_for":["designed for sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"025","subject":"lstm","predicate":"is_a","object":"rnn_variant","quantifier":"universal","truth_value":"T","evidence_for":["long short-term memory"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"026","subject":"lstm","predicate":"uses","object":"gating_mechanism","quantifier":"universal","truth_value":"T","evidence_for":["forget gate, input gate, output gate"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"027","subject":"lstm","predicate":"mitigates","object":"vanishing_gradient","quantifier":"universal","truth_value":"T","evidence_for":["gates retain long-term dependencies"],"evidence_against":["still struggles with very long sequences"],"source":"web_search","generation":0},
    {"id":"028","subject":"cnn","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["convolutional neural network"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"029","subject":"cnn","predicate":"specialized_for","object":"spatial_data","quantifier":"universal","truth_value":"T","evidence_for":["images, grid-like data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"030","subject":"cnn","predicate":"uses","object":"convolutional_layer","quantifier":"universal","truth_value":"T","evidence_for":["filters slide over input"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"031","subject":"cnn","predicate":"uses","object":"pooling_layer","quantifier":"universal","truth_value":"T","evidence_for":["downsampling"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"032","subject":"cnn","predicate":"captures","object":"local_spatial_features","quantifier":"universal","truth_value":"T","evidence_for":["edges, textures, shapes hierarchically"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"033","subject":"feedforward_network","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["data flows one direction"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"034","subject":"feedforward_network","predicate":"does_not_use","object":"recurrence","quantifier":"universal","truth_value":"T","evidence_for":["no feedback loops"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"035","subject":"perceptron","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["simplest, single layer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"036","subject":"perceptron","predicate":"cannot_solve","object":"nonlinear_problems","quantifier":"universal","truth_value":"T","evidence_for":["Minsky & Papert 1969, XOR problem"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"037","subject":"gan","predicate":"is_a","object":"generative_model","quantifier":"universal","truth_value":"T","evidence_for":["generative adversarial network"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"038","subject":"gan","predicate":"has_component","object":"generator","quantifier":"universal","truth_value":"T","evidence_for":["creates synthetic data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"039","subject":"gan","predicate":"has_component","object":"discriminator","quantifier":"universal","truth_value":"T","evidence_for":["distinguishes real from fake"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"040","subject":"gan","predicate":"trains_via","object":"adversarial_competition","quantifier":"universal","truth_value":"T","evidence_for":["generator vs discriminator"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"041","subject":"llm","predicate":"is_a","object":"language_model","quantifier":"universal","truth_value":"T","evidence_for":["large language model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"042","subject":"llm","predicate":"defined_by","object":"large_scale_training","quantifier":"universal","truth_value":"T","evidence_for":["billions of parameters, massive corpora"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"043","subject":"llm","predicate":"typically_uses","object":"transformer_architecture","quantifier":"common","truth_value":"T","evidence_for":["GPT, Claude, Gemini, LLaMA"],"evidence_against":["Mamba, RWKV are alternatives"],"source":"web_search","generation":0},
    {"id":"044","subject":"llm","predicate":"learns_via","object":"next_token_prediction","quantifier":"universal","truth_value":"T","evidence_for":["self-supervised pretraining objective"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"045","subject":"llm","predicate":"suffers_from","object":"hallucination","quantifier":"common","truth_value":"T","evidence_for":["generates plausible but false information"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"046","subject":"llm","predicate":"has","object":"knowledge_cutoff","quantifier":"universal","truth_value":"T","evidence_for":["training data has temporal boundary"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"047","subject":"pretraining","predicate":"is_a","object":"training_phase","quantifier":"universal","truth_value":"T","evidence_for":["first phase: general language patterns"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"048","subject":"pretraining","predicate":"uses","object":"self_supervised_learning","quantifier":"universal","truth_value":"T","evidence_for":["no labeled data, learns from raw text"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"049","subject":"fine_tuning","predicate":"is_a","object":"training_phase","quantifier":"universal","truth_value":"T","evidence_for":["adapt to specific tasks"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"050","subject":"fine_tuning","predicate":"requires","object":"smaller_labeled_dataset","quantifier":"universal","truth_value":"T","evidence_for":["domain-specific data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"051","subject":"rlhf","predicate":"is_a","object":"alignment_technique","quantifier":"universal","truth_value":"T","evidence_for":["reinforcement learning from human feedback"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"052","subject":"rlhf","predicate":"uses","object":"reward_model","quantifier":"universal","truth_value":"T","evidence_for":["trained on human preference rankings"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"053","subject":"rlhf","predicate":"aligns","object":"human_preferences","quantifier":"universal","truth_value":"T","evidence_for":["helpful, harmless, honest"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"054","subject":"reward_model","predicate":"outputs","object":"scalar_reward","quantifier":"universal","truth_value":"T","evidence_for":["numeric preference score"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"055","subject":"rlhf","predicate":"uses","object":"kl_divergence_penalty","quantifier":"universal","truth_value":"T","evidence_for":["prevents diverging too far from base model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"056","subject":"ppo","predicate":"is_a","object":"rl_algorithm","quantifier":"universal","truth_value":"T","evidence_for":["proximal policy optimization"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"057","subject":"ppo","predicate":"used_in","object":"rlhf","quantifier":"common","truth_value":"T","evidence_for":["used by OpenAI for ChatGPT"],"evidence_against":["DPO, GRPO are alternatives"],"source":"web_search","generation":0},
    {"id":"058","subject":"dpo","predicate":"is_a","object":"alignment_technique","quantifier":"universal","truth_value":"T","evidence_for":["direct preference optimization"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"059","subject":"dpo","predicate":"does_not_use","object":"reward_model","quantifier":"universal","truth_value":"T","evidence_for":["removes explicit RL, trains directly on preferences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"060","subject":"tokenization","predicate":"is_a","object":"preprocessing_step","quantifier":"universal","truth_value":"T","evidence_for":["converts text to numeric tokens"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"061","subject":"bpe","predicate":"is_a","object":"tokenization_method","quantifier":"universal","truth_value":"T","evidence_for":["byte pair encoding"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"062","subject":"embedding","predicate":"is_a","object":"vector_representation","quantifier":"universal","truth_value":"T","evidence_for":["dense numeric vectors"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"063","subject":"static_embedding","predicate":"cannot_handle","object":"polysemy","quantifier":"universal","truth_value":"T","evidence_for":["bank(river) = bank(money)"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"064","subject":"contextual_embedding","predicate":"handles","object":"polysemy","quantifier":"universal","truth_value":"T","evidence_for":["self-attention creates context-dependent vectors"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"065","subject":"transformer","predicate":"requires","object":"positional_encoding","quantifier":"universal","truth_value":"T","evidence_for":["no inherent sequence order"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"066","subject":"encoder_decoder","predicate":"is_a","object":"transformer_variant","quantifier":"universal","truth_value":"T","evidence_for":["original architecture, T5"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"067","subject":"decoder_only","predicate":"is_a","object":"transformer_variant","quantifier":"universal","truth_value":"T","evidence_for":["GPT family, causal LM"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"068","subject":"encoder_only","predicate":"is_a","object":"transformer_variant","quantifier":"universal","truth_value":"T","evidence_for":["BERT, bidirectional"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"069","subject":"gpt","predicate":"is_a","object":"decoder_only","quantifier":"universal","truth_value":"T","evidence_for":["generative pretrained transformer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"070","subject":"bert","predicate":"is_a","object":"encoder_only","quantifier":"universal","truth_value":"T","evidence_for":["bidirectional encoder representations"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"071","subject":"diffusion_model","predicate":"is_a","object":"generative_model","quantifier":"universal","truth_value":"T","evidence_for":["generates data by iterative denoising"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"072","subject":"diffusion_model","predicate":"uses","object":"forward_noise_process","quantifier":"universal","truth_value":"T","evidence_for":["gradually adds Gaussian noise to data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"073","subject":"diffusion_model","predicate":"uses","object":"reverse_denoising_process","quantifier":"universal","truth_value":"T","evidence_for":["learns to remove noise step by step"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"074","subject":"diffusion_model","predicate":"typically_uses","object":"unet_backbone","quantifier":"common","truth_value":"T","evidence_for":["U-Net for denoising prediction"],"evidence_against":["DiT uses transformer backbone"],"source":"web_search","generation":0},
    {"id":"075","subject":"diffusion_model","predicate":"outperforms","object":"gan","quantifier":"common","truth_value":"T","evidence_for":["higher quality, better mode coverage"],"evidence_against":["GANs faster at inference"],"source":"web_search","generation":0},
    {"id":"076","subject":"latent_diffusion","predicate":"is_a","object":"diffusion_model_variant","quantifier":"universal","truth_value":"T","evidence_for":["operates in compressed latent space"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"077","subject":"latent_diffusion","predicate":"uses","object":"vae_encoder","quantifier":"universal","truth_value":"T","evidence_for":["compresses images to latent space"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"078","subject":"latent_diffusion","predicate":"faster_than","object":"pixel_diffusion","quantifier":"universal","truth_value":"T","evidence_for":["2.7x speedup, trains 3x faster"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"079","subject":"stable_diffusion","predicate":"is_a","object":"latent_diffusion","quantifier":"universal","truth_value":"T","evidence_for":["Stability AI, 860M param denoiser"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"080","subject":"dall_e_2","predicate":"is_a","object":"diffusion_model","quantifier":"universal","truth_value":"T","evidence_for":["3.5B cascaded diffusion, uses CLIP"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"081","subject":"sora","predicate":"is_a","object":"diffusion_transformer","quantifier":"universal","truth_value":"T","evidence_for":["DiT architecture for video"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"082","subject":"diffusion_model","predicate":"slow_at","object":"inference","quantifier":"common","truth_value":"T","evidence_for":["requires many denoising steps"],"evidence_against":["distillation methods reduce steps"],"source":"web_search","generation":0},
    {"id":"083","subject":"gan","predicate":"faster_at","object":"inference","quantifier":"universal","truth_value":"T","evidence_for":["single forward pass generation"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"084","subject":"gan","predicate":"suffers_from","object":"mode_collapse","quantifier":"common","truth_value":"T","evidence_for":["generator produces limited variety"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"085","subject":"diffusion_model","predicate":"does_not_suffer_from","object":"mode_collapse","quantifier":"universal","truth_value":"T","evidence_for":["strong sample diversity, faithful mode coverage"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"086","subject":"rag","predicate":"is_a","object":"augmentation_technique","quantifier":"universal","truth_value":"T","evidence_for":["retrieval augmented generation"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"087","subject":"rag","predicate":"mitigates","object":"hallucination","quantifier":"partial","truth_value":"T","evidence_for":["grounds responses in retrieved docs"],"evidence_against":["LLM can still hallucinate around source material"],"source":"web_search","generation":0},
    {"id":"088","subject":"rag","predicate":"mitigates","object":"knowledge_cutoff","quantifier":"universal","truth_value":"T","evidence_for":["can access current information"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"089","subject":"rag","predicate":"uses","object":"vector_database","quantifier":"common","truth_value":"T","evidence_for":["stores document embeddings for retrieval"],"evidence_against":["can also use knowledge graphs"],"source":"web_search","generation":0},
    {"id":"090","subject":"rag","predicate":"does_not_require","object":"model_retraining","quantifier":"universal","truth_value":"T","evidence_for":["update knowledge by updating external store"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"091","subject":"knowledge_graph","predicate":"is_a","object":"data_structure","quantifier":"universal","truth_value":"T","evidence_for":["nodes connected by typed edges"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"092","subject":"knowledge_graph","predicate":"stores","object":"structured_relationships","quantifier":"universal","truth_value":"T","evidence_for":["entity → relation → entity triples"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"093","subject":"graphrag","predicate":"is_a","object":"rag_variant","quantifier":"universal","truth_value":"T","evidence_for":["uses knowledge graph for retrieval"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"094","subject":"graphrag","predicate":"enables","object":"multi_hop_reasoning","quantifier":"universal","truth_value":"T","evidence_for":["traverse graph edges for connected facts"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"095","subject":"vector_database","predicate":"stores","object":"embedding_vectors","quantifier":"universal","truth_value":"T","evidence_for":["dense vector representations for similarity search"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"096","subject":"mamba","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["selective state space model, 2023"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"097","subject":"mamba","predicate":"is_a","object":"ssm_variant","quantifier":"universal","truth_value":"T","evidence_for":["based on structured state space models"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"098","subject":"mamba","predicate":"does_not_use","object":"self_attention","quantifier":"universal","truth_value":"T","evidence_for":["replaces attention with selective state spaces"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"099","subject":"mamba","predicate":"has_complexity","object":"linear_in_sequence_length","quantifier":"universal","truth_value":"T","evidence_for":["O(n) vs transformer O(n^2)"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"100","subject":"mamba","predicate":"matches_performance_of","object":"transformer","quantifier":"observed","truth_value":"T","evidence_for":["comparable benchmarks at same scale"],"evidence_against":["transformers still dominant at largest scales"],"source":"web_search","generation":0},
    {"id":"101","subject":"mamba","predicate":"uses","object":"selective_scan","quantifier":"universal","truth_value":"T","evidence_for":["input-dependent parameters, content-based filtering"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"102","subject":"mamba","predicate":"faster_than","object":"transformer","quantifier":"observed","truth_value":"T","evidence_for":["5x inference throughput"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"103","subject":"ssm","predicate":"is_a","object":"sequence_model","quantifier":"universal","truth_value":"T","evidence_for":["state space model from control theory"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"104","subject":"ssm","predicate":"uses","object":"hidden_state","quantifier":"universal","truth_value":"T","evidence_for":["compresses history into fixed-size state"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"105","subject":"ssm","predicate":"processes","object":"sequential_data","quantifier":"universal","truth_value":"T","evidence_for":["designed for sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"106","subject":"transformer","predicate":"does_not_compress","object":"history","quantifier":"universal","truth_value":"T","evidence_for":["full attention matrix retains all token info"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"107","subject":"ssm","predicate":"compresses","object":"history","quantifier":"universal","truth_value":"T","evidence_for":["history folded into small state vector"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"108","subject":"mamba","predicate":"selectively_compresses","object":"history","quantifier":"universal","truth_value":"T","evidence_for":["input-dependent A,B matrices choose what to retain"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"109","subject":"ai_agent","predicate":"is_a","object":"ai_system","quantifier":"universal","truth_value":"T","evidence_for":["autonomous system that plans, reasons, acts"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"110","subject":"ai_agent","predicate":"uses","object":"tool_calling","quantifier":"common","truth_value":"T","evidence_for":["invokes external APIs, search, code execution"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"111","subject":"ai_agent","predicate":"uses","object":"planning","quantifier":"universal","truth_value":"T","evidence_for":["decomposes goals into steps"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"112","subject":"ai_agent","predicate":"typically_uses","object":"llm","quantifier":"common","truth_value":"T","evidence_for":["LLM as reasoning core"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"113","subject":"transfer_learning","predicate":"is_a","object":"training_technique","quantifier":"universal","truth_value":"T","evidence_for":["reuse pretrained model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"114","subject":"transfer_learning","predicate":"reduces","object":"training_cost","quantifier":"universal","truth_value":"T","evidence_for":["leverages existing knowledge"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"115","subject":"overfitting","predicate":"is_a","object":"training_problem","quantifier":"universal","truth_value":"T","evidence_for":["memorizes training data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"116","subject":"dropout","predicate":"prevents","object":"overfitting","quantifier":"partial","truth_value":"T","evidence_for":["randomly deactivates neurons"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"117","subject":"residual_connection","predicate":"mitigates","object":"vanishing_gradient","quantifier":"universal","truth_value":"T","evidence_for":["skip connections, ResNet 2015"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"118","subject":"transformer","predicate":"uses","object":"residual_connection","quantifier":"universal","truth_value":"T","evidence_for":["around each sublayer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"119","subject":"transformer","predicate":"uses","object":"layer_normalization","quantifier":"universal","truth_value":"T","evidence_for":["stabilizes training"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"120","subject":"vae","predicate":"is_a","object":"generative_model","quantifier":"universal","truth_value":"T","evidence_for":["variational autoencoder"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"121","subject":"vae","predicate":"uses","object":"latent_space","quantifier":"universal","truth_value":"T","evidence_for":["encodes to compressed representation"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"122","subject":"deep_learning","predicate":"is_a","object":"machine_learning_subset","quantifier":"universal","truth_value":"T","evidence_for":["multiple layers"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"123","subject":"machine_learning","predicate":"is_a","object":"ai_subset","quantifier":"universal","truth_value":"T","evidence_for":["learning from data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"124","subject":"supervised_learning","predicate":"is_a","object":"ml_paradigm","quantifier":"universal","truth_value":"T","evidence_for":["labeled input-output pairs"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"125","subject":"unsupervised_learning","predicate":"is_a","object":"ml_paradigm","quantifier":"universal","truth_value":"T","evidence_for":["find patterns without labels"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"126","subject":"reinforcement_learning","predicate":"is_a","object":"ml_paradigm","quantifier":"universal","truth_value":"T","evidence_for":["learn via reward signals"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"127","subject":"hallucination","predicate":"is_a","object":"llm_failure_mode","quantifier":"universal","truth_value":"T","evidence_for":["generates false but plausible text"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"128","subject":"hallucination","predicate":"caused_by","object":"pattern_completion_without_grounding","quantifier":"primary","truth_value":"T","evidence_for":["statistical patterns, no fact verification"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"129","subject":"attention_mechanism","predicate":"analogous_to","object":"selective_focus","quantifier":"conceptual","truth_value":"T","evidence_for":["cocktail party problem: focus on relevant signal"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"130","subject":"mamba","predicate":"analogous_to","object":"selective_memory","quantifier":"conceptual","truth_value":"T","evidence_for":["choose what to remember/forget from state"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"131","subject":"transformer","predicate":"remembers","object":"everything_in_context","quantifier":"universal","truth_value":"T","evidence_for":["full attention over all tokens"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"132","subject":"rnn","predicate":"remembers","object":"compressed_history","quantifier":"universal","truth_value":"T","evidence_for":["fixed hidden state"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"133","subject":"transformer","predicate":"limited_by","object":"context_window","quantifier":"universal","truth_value":"T","evidence_for":["fixed max sequence length"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"134","subject":"mamba","predicate":"not_limited_by","object":"context_window","quantifier":"universal","truth_value":"T","evidence_for":["handles million+ length sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"135","subject":"diffusion_model","predicate":"inspired_by","object":"thermodynamics","quantifier":"universal","truth_value":"T","evidence_for":["nonequilibrium statistical physics"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"136","subject":"ssm","predicate":"inspired_by","object":"control_theory","quantifier":"universal","truth_value":"T","evidence_for":["continuous-time dynamical systems"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"137","subject":"neural_network","predicate":"inspired_by","object":"biological_neurons","quantifier":"universal","truth_value":"T","evidence_for":["original motivation"],"evidence_against":["modern NNs diverge significantly from biology"],"source":"web_search","generation":0},
    {"id":"138","subject":"lora","predicate":"is_a","object":"fine_tuning_technique","quantifier":"universal","truth_value":"T","evidence_for":["low-rank adaptation, trains small adapters"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"139","subject":"lora","predicate":"reduces","object":"training_cost","quantifier":"universal","truth_value":"T","evidence_for":["trains thousands vs billions of params"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"140","subject":"prompt_engineering","predicate":"is_a","object":"inference_technique","quantifier":"universal","truth_value":"T","evidence_for":["crafting inputs to guide model behavior"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"141","subject":"prompt_engineering","predicate":"does_not_modify","object":"model_weights","quantifier":"universal","truth_value":"T","evidence_for":["only changes input, not model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"142","subject":"in_context_learning","predicate":"is_a","object":"emergent_capability","quantifier":"universal","truth_value":"T","evidence_for":["learn from examples in prompt without training"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"143","subject":"mamba2","predicate":"is_a","object":"mamba_variant","quantifier":"universal","truth_value":"T","evidence_for":["2-8x faster, state space duality framework"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"144","subject":"mamba2","predicate":"proves","object":"transformers_are_ssms","quantifier":"theoretical","truth_value":"T","evidence_for":["structured semiseparable matrices connect both"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"145","subject":"attention_mechanism","predicate":"is_a","object":"communication_mechanism","quantifier":"universal","truth_value":"T","evidence_for":["tokens share information"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"146","subject":"mlp","predicate":"is_a","object":"computation_mechanism","quantifier":"universal","truth_value":"T","evidence_for":["independent per-token processing"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"147","subject":"transformer","predicate":"alternates","object":"communication_and_computation","quantifier":"universal","truth_value":"T","evidence_for":["attention (communicate) + FFN (compute) per layer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"148","subject":"mamba","predicate":"merges","object":"communication_and_computation","quantifier":"universal","truth_value":"T","evidence_for":["single block handles both via SSM + projections"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"149","subject":"diffusion_model","predicate":"uses","object":"iterative_refinement","quantifier":"universal","truth_value":"T","evidence_for":["many small denoising steps"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"150","subject":"rag","predicate":"combines","object":"retrieval_and_generation","quantifier":"universal","truth_value":"T","evidence_for":["retrieve relevant docs then generate answer"],"evidence_against":[],"source":"web_search","generation":0}
  ]
}
