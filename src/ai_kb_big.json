{
  "topic": "artificial_intelligence_comprehensive",
  "default_context": "computer_science",
  "concepts": [
    {"subject": "neural_network", "context": "computer_science", "definition": "computational model inspired by biological neurons that learns patterns from data"},
    {"subject": "neuron", "context": "computer_science", "definition": "basic computational unit in a neural network that applies a weighted sum and activation function"},
    {"subject": "activation_function", "context": "computer_science", "definition": "nonlinear function applied to neuron output, preventing layer collapse to a single linear transform"},
    {"subject": "relu", "context": "computer_science", "definition": "rectified linear unit activation function that outputs max(0, x), most widely used in deep learning"},
    {"subject": "sigmoid", "context": "computer_science", "definition": "activation function that maps inputs to the range (0, 1), used in binary classification and gating"},
    {"subject": "backpropagation", "context": "computer_science", "definition": "training algorithm that propagates error gradients backward through a neural network to update weights"},
    {"subject": "gradient_descent", "context": "computer_science", "definition": "optimization algorithm that iteratively adjusts parameters in the direction that minimizes a loss function"},
    {"subject": "transformer", "context": "computer_science", "definition": "neural network architecture using self-attention, introduced in Attention Is All You Need (2017)"},
    {"subject": "self_attention", "context": "computer_science", "definition": "mechanism where each token computes relevance scores against all other tokens via query-key-value matrices"},
    {"subject": "multi_head_attention", "context": "computer_science", "definition": "parallel self-attention heads that capture different relationship types (syntactic, semantic, positional)"},
    {"subject": "rnn", "context": "computer_science", "definition": "recurrent neural network that processes sequences using feedback loops and hidden state"},
    {"subject": "lstm", "context": "computer_science", "definition": "long short-term memory network using forget/input/output gates to retain long-range dependencies"},
    {"subject": "cnn", "context": "computer_science", "definition": "convolutional neural network specialized for spatial data using learned filters and pooling layers"},
    {"subject": "feedforward_network", "context": "computer_science", "definition": "neural network where data flows in one direction with no feedback loops or recurrence"},
    {"subject": "perceptron", "context": "computer_science", "definition": "simplest single-layer neural network, limited to linear decision boundaries (Rosenblatt, 1958)"},
    {"subject": "gan", "context": "computer_science", "definition": "generative adversarial network where a generator and discriminator compete to produce realistic data"},
    {"subject": "llm", "context": "computer_science", "definition": "large language model with billions of parameters trained on massive text corpora for general language tasks"},
    {"subject": "pretraining", "context": "computer_science", "definition": "initial self-supervised training phase where a model learns general language patterns from raw text"},
    {"subject": "fine_tuning", "context": "computer_science", "definition": "second training phase that adapts a pretrained model to a specific task using a smaller labeled dataset"},
    {"subject": "rlhf", "context": "computer_science", "definition": "reinforcement learning from human feedback, an alignment technique using human preference rankings"},
    {"subject": "reward_model", "context": "computer_science", "definition": "model trained on human preference data that outputs a scalar score for ranking LLM responses"},
    {"subject": "ppo", "context": "computer_science", "definition": "proximal policy optimization, a stable reinforcement learning algorithm commonly used in RLHF"},
    {"subject": "dpo", "context": "computer_science", "definition": "direct preference optimization, an alignment technique that trains directly on preferences without a reward model"},
    {"subject": "tokenization", "context": "computer_science", "definition": "preprocessing step that converts raw text into discrete numeric tokens for model consumption"},
    {"subject": "bpe", "context": "computer_science", "definition": "byte pair encoding, a subword tokenization method that iteratively merges frequent character pairs"},
    {"subject": "embedding", "context": "computer_science", "definition": "dense vector representation that maps discrete tokens into continuous high-dimensional space"},
    {"subject": "static_embedding", "context": "computer_science", "definition": "fixed word vector (e.g. Word2Vec, GloVe) that assigns one representation per word regardless of context"},
    {"subject": "contextual_embedding", "context": "computer_science", "definition": "context-dependent word vector produced by attention-based models, handling polysemy and ambiguity"},
    {"subject": "encoder_decoder", "context": "computer_science", "definition": "transformer variant with both encoder and decoder stacks, used for sequence-to-sequence tasks (e.g. T5)"},
    {"subject": "decoder_only", "context": "computer_science", "definition": "transformer variant using only masked self-attention for autoregressive generation (e.g. GPT family)"},
    {"subject": "encoder_only", "context": "computer_science", "definition": "transformer variant using only bidirectional self-attention for representation learning (e.g. BERT)"},
    {"subject": "gpt", "context": "computer_science", "definition": "generative pretrained transformer, a decoder-only LLM architecture by OpenAI"},
    {"subject": "bert", "context": "computer_science", "definition": "bidirectional encoder representations from transformers, an encoder-only model for language understanding"},
    {"subject": "diffusion_model", "context": "computer_science", "definition": "generative model that learns to produce data by iteratively denoising from random noise"},
    {"subject": "latent_diffusion", "context": "computer_science", "definition": "diffusion model operating in compressed latent space via a VAE encoder for faster training and inference"},
    {"subject": "stable_diffusion", "context": "computer_science", "definition": "open-source latent diffusion model by Stability AI for text-to-image generation"},
    {"subject": "dall_e_2", "context": "computer_science", "definition": "OpenAI diffusion model using CLIP guidance for text-to-image generation (3.5B parameters)"},
    {"subject": "sora", "context": "computer_science", "definition": "OpenAI diffusion transformer (DiT) architecture for text-to-video generation"},
    {"subject": "rag", "context": "computer_science", "definition": "retrieval-augmented generation, combining document retrieval with LLM generation to ground responses"},
    {"subject": "knowledge_graph", "context": "computer_science", "definition": "graph data structure storing entity-relation-entity triples for structured knowledge representation"},
    {"subject": "graphrag", "context": "computer_science", "definition": "RAG variant using knowledge graph traversal instead of vector similarity for multi-hop reasoning"},
    {"subject": "vector_database", "context": "computer_science", "definition": "database optimized for storing and querying high-dimensional embedding vectors via similarity search"},
    {"subject": "mamba", "context": "computer_science", "definition": "selective state space model architecture for sequence modeling with linear-time complexity (Gu & Dao, 2023)"},
    {"subject": "ssm", "context": "computer_science", "definition": "state space model, a sequence model derived from control theory using continuous-time dynamical systems"},
    {"subject": "ai_agent", "context": "computer_science", "definition": "autonomous AI system that plans, reasons, and acts using tools to accomplish goals"},
    {"subject": "transfer_learning", "context": "computer_science", "definition": "training technique that reuses a pretrained model's knowledge to reduce cost on new tasks"},
    {"subject": "overfitting", "context": "computer_science", "definition": "training failure where a model memorizes training data and loses ability to generalize to new inputs"},
    {"subject": "dropout", "context": "computer_science", "definition": "regularization technique that randomly deactivates neurons during training to prevent overfitting"},
    {"subject": "residual_connection", "context": "computer_science", "definition": "skip connection that adds a layer's input to its output, mitigating vanishing gradients (ResNet, 2015)"},
    {"subject": "vae", "context": "computer_science", "definition": "variational autoencoder, a generative model that learns a continuous latent space via encoder-decoder structure"},
    {"subject": "deep_learning", "context": "computer_science", "definition": "subset of machine learning using neural networks with multiple layers to learn hierarchical representations"},
    {"subject": "machine_learning", "context": "computer_science", "definition": "subset of AI where systems learn patterns and make decisions from data without explicit programming"},
    {"subject": "supervised_learning", "context": "computer_science", "definition": "ML paradigm where models learn from labeled input-output pairs to predict outputs for new inputs"},
    {"subject": "unsupervised_learning", "context": "computer_science", "definition": "ML paradigm where models discover patterns and structure in unlabeled data without supervision"},
    {"subject": "reinforcement_learning", "context": "computer_science", "definition": "ML paradigm where an agent learns optimal behavior through trial-and-error reward signals"},
    {"subject": "hallucination", "context": "computer_science", "definition": "LLM failure mode where the model generates plausible but factually incorrect or fabricated text"},
    {"subject": "attention_mechanism", "context": "computer_science", "definition": "neural network mechanism enabling tokens to selectively focus on relevant parts of the input sequence"},
    {"subject": "lora", "context": "computer_science", "definition": "low-rank adaptation, a parameter-efficient fine-tuning method that trains small adapter matrices"},
    {"subject": "prompt_engineering", "context": "computer_science", "definition": "technique of crafting model inputs to guide LLM behavior without modifying model weights"},
    {"subject": "in_context_learning", "context": "computer_science", "definition": "emergent LLM capability to learn from few-shot examples provided in the prompt without weight updates"},
    {"subject": "mamba2", "context": "computer_science", "definition": "improved Mamba variant proving state space duality with transformers, 2-8x faster than Mamba 1"},
    {"subject": "mlp", "context": "computer_science", "definition": "multi-layer perceptron, a feedforward network performing independent per-token computation in transformers"}
  ],
  "entries": [
    {"id":"001","subject":"neural_network","context":"computer_science","predicate":"is_a","object":"computational_model","quantifier":"universal","truth_value":"T","evidence_for":["mathematical model inspired by biological neurons"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"002","subject":"neural_network","context":"computer_science","predicate":"has_component","object":"input_layer","quantifier":"universal","truth_value":"T","evidence_for":["receives input data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"003","subject":"neural_network","context":"computer_science","predicate":"has_component","object":"hidden_layer","quantifier":"possible","truth_value":"T","evidence_for":["intermediate processing, 0 or more"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"004","subject":"neural_network","context":"computer_science","predicate":"has_component","object":"output_layer","quantifier":"universal","truth_value":"T","evidence_for":["produces final result"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"005","subject":"neuron","context":"computer_science","predicate":"performs","object":"weighted_sum","quantifier":"universal","truth_value":"T","evidence_for":["sum of inputs * weights + bias"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"006","subject":"neuron","context":"computer_science","predicate":"applies","object":"activation_function","quantifier":"universal","truth_value":"T","evidence_for":["introduces nonlinearity"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"007","subject":"activation_function","context":"computer_science","predicate":"provides","object":"nonlinearity","quantifier":"universal","truth_value":"T","evidence_for":["without it layers collapse to single linear transform"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"008","subject":"relu","context":"computer_science","predicate":"is_a","object":"activation_function","quantifier":"universal","truth_value":"T","evidence_for":["rectified linear unit, most popular"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"009","subject":"sigmoid","context":"computer_science","predicate":"is_a","object":"activation_function","quantifier":"universal","truth_value":"T","evidence_for":["maps to 0-1"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"010","subject":"backpropagation","context":"computer_science","predicate":"is_a","object":"training_algorithm","quantifier":"universal","truth_value":"T","evidence_for":["propagates error backward"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"011","subject":"backpropagation","context":"computer_science","predicate":"uses","object":"gradient_descent","quantifier":"universal","truth_value":"T","evidence_for":["calculates gradient of loss"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"012","subject":"gradient_descent","context":"computer_science","predicate":"is_a","object":"optimization_algorithm","quantifier":"universal","truth_value":"T","evidence_for":["iteratively minimizes loss"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"013","subject":"transformer","context":"computer_science","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["Attention Is All You Need 2017"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"014","subject":"transformer","context":"computer_science","predicate":"uses","object":"self_attention","quantifier":"universal","truth_value":"T","evidence_for":["core mechanism"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"015","subject":"transformer","context":"computer_science","predicate":"does_not_use","object":"recurrence","quantifier":"universal","truth_value":"T","evidence_for":["eliminated RNN sequential processing"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"016","subject":"self_attention","context":"computer_science","predicate":"computes","object":"query_key_value_matrices","quantifier":"universal","truth_value":"T","evidence_for":["Q K V via learned weight matrices"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"017","subject":"self_attention","context":"computer_science","predicate":"enables","object":"parallel_processing","quantifier":"universal","truth_value":"T","evidence_for":["all tokens simultaneously"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"018","subject":"self_attention","context":"computer_science","predicate":"has_complexity","object":"quadratic_in_sequence_length","quantifier":"universal","truth_value":"T","evidence_for":["O(n^2) in context window"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"019","subject":"multi_head_attention","context":"computer_science","predicate":"is_a","object":"self_attention_variant","quantifier":"universal","truth_value":"T","evidence_for":["multiple parallel attention heads"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"020","subject":"multi_head_attention","context":"computer_science","predicate":"captures","object":"multiple_relationship_types","quantifier":"universal","truth_value":"T","evidence_for":["syntactic, semantic, positional in parallel"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"021","subject":"rnn","context":"computer_science","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["recurrent neural network"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"022","subject":"rnn","context":"computer_science","predicate":"uses","object":"recurrence","quantifier":"universal","truth_value":"T","evidence_for":["feedback loops, sequential"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"023","subject":"rnn","context":"computer_science","predicate":"suffers_from","object":"vanishing_gradient","quantifier":"common","truth_value":"T","evidence_for":["gradients diminish through long sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"024","subject":"rnn","context":"computer_science","predicate":"processes","object":"sequential_data","quantifier":"universal","truth_value":"T","evidence_for":["designed for sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"025","subject":"lstm","context":"computer_science","predicate":"is_a","object":"rnn_variant","quantifier":"universal","truth_value":"T","evidence_for":["long short-term memory"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"026","subject":"lstm","context":"computer_science","predicate":"uses","object":"gating_mechanism","quantifier":"universal","truth_value":"T","evidence_for":["forget gate, input gate, output gate"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"027","subject":"lstm","context":"computer_science","predicate":"mitigates","object":"vanishing_gradient","quantifier":"universal","truth_value":"T","evidence_for":["gates retain long-term dependencies"],"evidence_against":["still struggles with very long sequences"],"source":"web_search","generation":0},
    {"id":"028","subject":"cnn","context":"computer_science","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["convolutional neural network"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"029","subject":"cnn","context":"computer_science","predicate":"specialized_for","object":"spatial_data","quantifier":"universal","truth_value":"T","evidence_for":["images, grid-like data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"030","subject":"cnn","context":"computer_science","predicate":"uses","object":"convolutional_layer","quantifier":"universal","truth_value":"T","evidence_for":["filters slide over input"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"031","subject":"cnn","context":"computer_science","predicate":"uses","object":"pooling_layer","quantifier":"universal","truth_value":"T","evidence_for":["downsampling"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"032","subject":"cnn","context":"computer_science","predicate":"captures","object":"local_spatial_features","quantifier":"universal","truth_value":"T","evidence_for":["edges, textures, shapes hierarchically"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"033","subject":"feedforward_network","context":"computer_science","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["data flows one direction"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"034","subject":"feedforward_network","context":"computer_science","predicate":"does_not_use","object":"recurrence","quantifier":"universal","truth_value":"T","evidence_for":["no feedback loops"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"035","subject":"perceptron","context":"computer_science","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["simplest, single layer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"036","subject":"perceptron","context":"computer_science","predicate":"cannot_solve","object":"nonlinear_problems","quantifier":"universal","truth_value":"T","evidence_for":["Minsky & Papert 1969, XOR problem"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"037","subject":"gan","context":"computer_science","predicate":"is_a","object":"generative_model","quantifier":"universal","truth_value":"T","evidence_for":["generative adversarial network"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"038","subject":"gan","context":"computer_science","predicate":"has_component","object":"generator","quantifier":"universal","truth_value":"T","evidence_for":["creates synthetic data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"039","subject":"gan","context":"computer_science","predicate":"has_component","object":"discriminator","quantifier":"universal","truth_value":"T","evidence_for":["distinguishes real from fake"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"040","subject":"gan","context":"computer_science","predicate":"trains_via","object":"adversarial_competition","quantifier":"universal","truth_value":"T","evidence_for":["generator vs discriminator"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"041","subject":"llm","context":"computer_science","predicate":"is_a","object":"language_model","quantifier":"universal","truth_value":"T","evidence_for":["large language model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"042","subject":"llm","context":"computer_science","predicate":"defined_by","object":"large_scale_training","quantifier":"universal","truth_value":"T","evidence_for":["billions of parameters, massive corpora"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"043","subject":"llm","context":"computer_science","predicate":"typically_uses","object":"transformer_architecture","quantifier":"common","truth_value":"T","evidence_for":["GPT, Claude, Gemini, LLaMA"],"evidence_against":["Mamba, RWKV are alternatives"],"source":"web_search","generation":0},
    {"id":"044","subject":"llm","context":"computer_science","predicate":"learns_via","object":"next_token_prediction","quantifier":"universal","truth_value":"T","evidence_for":["self-supervised pretraining objective"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"045","subject":"llm","context":"computer_science","predicate":"suffers_from","object":"hallucination","quantifier":"common","truth_value":"T","evidence_for":["generates plausible but false information"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"046","subject":"llm","context":"computer_science","predicate":"has","object":"knowledge_cutoff","quantifier":"universal","truth_value":"T","evidence_for":["training data has temporal boundary"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"047","subject":"pretraining","context":"computer_science","predicate":"is_a","object":"training_phase","quantifier":"universal","truth_value":"T","evidence_for":["first phase: general language patterns"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"048","subject":"pretraining","context":"computer_science","predicate":"uses","object":"self_supervised_learning","quantifier":"universal","truth_value":"T","evidence_for":["no labeled data, learns from raw text"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"049","subject":"fine_tuning","context":"computer_science","predicate":"is_a","object":"training_phase","quantifier":"universal","truth_value":"T","evidence_for":["adapt to specific tasks"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"050","subject":"fine_tuning","context":"computer_science","predicate":"requires","object":"smaller_labeled_dataset","quantifier":"universal","truth_value":"T","evidence_for":["domain-specific data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"051","subject":"rlhf","context":"computer_science","predicate":"is_a","object":"alignment_technique","quantifier":"universal","truth_value":"T","evidence_for":["reinforcement learning from human feedback"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"052","subject":"rlhf","context":"computer_science","predicate":"uses","object":"reward_model","quantifier":"universal","truth_value":"T","evidence_for":["trained on human preference rankings"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"053","subject":"rlhf","context":"computer_science","predicate":"aligns","object":"human_preferences","quantifier":"universal","truth_value":"T","evidence_for":["helpful, harmless, honest"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"054","subject":"reward_model","context":"computer_science","predicate":"outputs","object":"scalar_reward","quantifier":"universal","truth_value":"T","evidence_for":["numeric preference score"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"055","subject":"rlhf","context":"computer_science","predicate":"uses","object":"kl_divergence_penalty","quantifier":"universal","truth_value":"T","evidence_for":["prevents diverging too far from base model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"056","subject":"ppo","context":"computer_science","predicate":"is_a","object":"rl_algorithm","quantifier":"universal","truth_value":"T","evidence_for":["proximal policy optimization"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"057","subject":"ppo","context":"computer_science","predicate":"used_in","object":"rlhf","quantifier":"common","truth_value":"T","evidence_for":["used by OpenAI for ChatGPT"],"evidence_against":["DPO, GRPO are alternatives"],"source":"web_search","generation":0},
    {"id":"058","subject":"dpo","context":"computer_science","predicate":"is_a","object":"alignment_technique","quantifier":"universal","truth_value":"T","evidence_for":["direct preference optimization"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"059","subject":"dpo","context":"computer_science","predicate":"does_not_use","object":"reward_model","quantifier":"universal","truth_value":"T","evidence_for":["removes explicit RL, trains directly on preferences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"060","subject":"tokenization","context":"computer_science","predicate":"is_a","object":"preprocessing_step","quantifier":"universal","truth_value":"T","evidence_for":["converts text to numeric tokens"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"061","subject":"bpe","context":"computer_science","predicate":"is_a","object":"tokenization_method","quantifier":"universal","truth_value":"T","evidence_for":["byte pair encoding"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"062","subject":"embedding","context":"computer_science","predicate":"is_a","object":"vector_representation","quantifier":"universal","truth_value":"T","evidence_for":["dense numeric vectors"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"063","subject":"static_embedding","context":"computer_science","predicate":"cannot_handle","object":"polysemy","quantifier":"universal","truth_value":"T","evidence_for":["bank(river) = bank(money)"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"064","subject":"contextual_embedding","context":"computer_science","predicate":"handles","object":"polysemy","quantifier":"universal","truth_value":"T","evidence_for":["self-attention creates context-dependent vectors"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"065","subject":"transformer","context":"computer_science","predicate":"requires","object":"positional_encoding","quantifier":"universal","truth_value":"T","evidence_for":["no inherent sequence order"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"066","subject":"encoder_decoder","context":"computer_science","predicate":"is_a","object":"transformer_variant","quantifier":"universal","truth_value":"T","evidence_for":["original architecture, T5"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"067","subject":"decoder_only","context":"computer_science","predicate":"is_a","object":"transformer_variant","quantifier":"universal","truth_value":"T","evidence_for":["GPT family, causal LM"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"068","subject":"encoder_only","context":"computer_science","predicate":"is_a","object":"transformer_variant","quantifier":"universal","truth_value":"T","evidence_for":["BERT, bidirectional"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"069","subject":"gpt","context":"computer_science","predicate":"is_a","object":"decoder_only","quantifier":"universal","truth_value":"T","evidence_for":["generative pretrained transformer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"070","subject":"bert","context":"computer_science","predicate":"is_a","object":"encoder_only","quantifier":"universal","truth_value":"T","evidence_for":["bidirectional encoder representations"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"071","subject":"diffusion_model","context":"computer_science","predicate":"is_a","object":"generative_model","quantifier":"universal","truth_value":"T","evidence_for":["generates data by iterative denoising"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"072","subject":"diffusion_model","context":"computer_science","predicate":"uses","object":"forward_noise_process","quantifier":"universal","truth_value":"T","evidence_for":["gradually adds Gaussian noise to data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"073","subject":"diffusion_model","context":"computer_science","predicate":"uses","object":"reverse_denoising_process","quantifier":"universal","truth_value":"T","evidence_for":["learns to remove noise step by step"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"074","subject":"diffusion_model","context":"computer_science","predicate":"typically_uses","object":"unet_backbone","quantifier":"common","truth_value":"T","evidence_for":["U-Net for denoising prediction"],"evidence_against":["DiT uses transformer backbone"],"source":"web_search","generation":0},
    {"id":"075","subject":"diffusion_model","context":"computer_science","predicate":"outperforms","object":"gan","quantifier":"common","truth_value":"T","evidence_for":["higher quality, better mode coverage"],"evidence_against":["GANs faster at inference"],"source":"web_search","generation":0},
    {"id":"076","subject":"latent_diffusion","context":"computer_science","predicate":"is_a","object":"diffusion_model_variant","quantifier":"universal","truth_value":"T","evidence_for":["operates in compressed latent space"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"077","subject":"latent_diffusion","context":"computer_science","predicate":"uses","object":"vae_encoder","quantifier":"universal","truth_value":"T","evidence_for":["compresses images to latent space"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"078","subject":"latent_diffusion","context":"computer_science","predicate":"faster_than","object":"pixel_diffusion","quantifier":"universal","truth_value":"T","evidence_for":["2.7x speedup, trains 3x faster"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"079","subject":"stable_diffusion","context":"computer_science","predicate":"is_a","object":"latent_diffusion","quantifier":"universal","truth_value":"T","evidence_for":["Stability AI, 860M param denoiser"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"080","subject":"dall_e_2","context":"computer_science","predicate":"is_a","object":"diffusion_model","quantifier":"universal","truth_value":"T","evidence_for":["3.5B cascaded diffusion, uses CLIP"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"081","subject":"sora","context":"computer_science","predicate":"is_a","object":"diffusion_transformer","quantifier":"universal","truth_value":"T","evidence_for":["DiT architecture for video"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"082","subject":"diffusion_model","context":"computer_science","predicate":"slow_at","object":"inference","quantifier":"common","truth_value":"T","evidence_for":["requires many denoising steps"],"evidence_against":["distillation methods reduce steps"],"source":"web_search","generation":0},
    {"id":"083","subject":"gan","context":"computer_science","predicate":"faster_at","object":"inference","quantifier":"universal","truth_value":"T","evidence_for":["single forward pass generation"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"084","subject":"gan","context":"computer_science","predicate":"suffers_from","object":"mode_collapse","quantifier":"common","truth_value":"T","evidence_for":["generator produces limited variety"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"085","subject":"diffusion_model","context":"computer_science","predicate":"does_not_suffer_from","object":"mode_collapse","quantifier":"universal","truth_value":"T","evidence_for":["strong sample diversity, faithful mode coverage"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"086","subject":"rag","context":"computer_science","predicate":"is_a","object":"augmentation_technique","quantifier":"universal","truth_value":"T","evidence_for":["retrieval augmented generation"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"087","subject":"rag","context":"computer_science","predicate":"mitigates","object":"hallucination","quantifier":"partial","truth_value":"T","evidence_for":["grounds responses in retrieved docs"],"evidence_against":["LLM can still hallucinate around source material"],"source":"web_search","generation":0},
    {"id":"088","subject":"rag","context":"computer_science","predicate":"mitigates","object":"knowledge_cutoff","quantifier":"universal","truth_value":"T","evidence_for":["can access current information"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"089","subject":"rag","context":"computer_science","predicate":"uses","object":"vector_database","quantifier":"common","truth_value":"T","evidence_for":["stores document embeddings for retrieval"],"evidence_against":["can also use knowledge graphs"],"source":"web_search","generation":0},
    {"id":"090","subject":"rag","context":"computer_science","predicate":"does_not_require","object":"model_retraining","quantifier":"universal","truth_value":"T","evidence_for":["update knowledge by updating external store"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"091","subject":"knowledge_graph","context":"computer_science","predicate":"is_a","object":"data_structure","quantifier":"universal","truth_value":"T","evidence_for":["nodes connected by typed edges"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"092","subject":"knowledge_graph","context":"computer_science","predicate":"stores","object":"structured_relationships","quantifier":"universal","truth_value":"T","evidence_for":["entity → relation → entity triples"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"093","subject":"graphrag","context":"computer_science","predicate":"is_a","object":"rag_variant","quantifier":"universal","truth_value":"T","evidence_for":["uses knowledge graph for retrieval"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"094","subject":"graphrag","context":"computer_science","predicate":"enables","object":"multi_hop_reasoning","quantifier":"universal","truth_value":"T","evidence_for":["traverse graph edges for connected facts"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"095","subject":"vector_database","context":"computer_science","predicate":"stores","object":"embedding_vectors","quantifier":"universal","truth_value":"T","evidence_for":["dense vector representations for similarity search"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"096","subject":"mamba","context":"computer_science","predicate":"is_a","object":"neural_network_architecture","quantifier":"universal","truth_value":"T","evidence_for":["selective state space model, 2023"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"097","subject":"mamba","context":"computer_science","predicate":"is_a","object":"ssm_variant","quantifier":"universal","truth_value":"T","evidence_for":["based on structured state space models"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"098","subject":"mamba","context":"computer_science","predicate":"does_not_use","object":"self_attention","quantifier":"universal","truth_value":"T","evidence_for":["replaces attention with selective state spaces"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"099","subject":"mamba","context":"computer_science","predicate":"has_complexity","object":"linear_in_sequence_length","quantifier":"universal","truth_value":"T","evidence_for":["O(n) vs transformer O(n^2)"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"100","subject":"mamba","context":"computer_science","predicate":"matches_performance_of","object":"transformer","quantifier":"observed","truth_value":"T","evidence_for":["comparable benchmarks at same scale"],"evidence_against":["transformers still dominant at largest scales"],"source":"web_search","generation":0},
    {"id":"101","subject":"mamba","context":"computer_science","predicate":"uses","object":"selective_scan","quantifier":"universal","truth_value":"T","evidence_for":["input-dependent parameters, content-based filtering"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"102","subject":"mamba","context":"computer_science","predicate":"faster_than","object":"transformer","quantifier":"observed","truth_value":"T","evidence_for":["5x inference throughput"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"103","subject":"ssm","context":"computer_science","predicate":"is_a","object":"sequence_model","quantifier":"universal","truth_value":"T","evidence_for":["state space model from control theory"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"104","subject":"ssm","context":"computer_science","predicate":"uses","object":"hidden_state","quantifier":"universal","truth_value":"T","evidence_for":["compresses history into fixed-size state"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"105","subject":"ssm","context":"computer_science","predicate":"processes","object":"sequential_data","quantifier":"universal","truth_value":"T","evidence_for":["designed for sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"106","subject":"transformer","context":"computer_science","predicate":"does_not_compress","object":"history","quantifier":"universal","truth_value":"T","evidence_for":["full attention matrix retains all token info"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"107","subject":"ssm","context":"computer_science","predicate":"compresses","object":"history","quantifier":"universal","truth_value":"T","evidence_for":["history folded into small state vector"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"108","subject":"mamba","context":"computer_science","predicate":"selectively_compresses","object":"history","quantifier":"universal","truth_value":"T","evidence_for":["input-dependent A,B matrices choose what to retain"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"109","subject":"ai_agent","context":"computer_science","predicate":"is_a","object":"ai_system","quantifier":"universal","truth_value":"T","evidence_for":["autonomous system that plans, reasons, acts"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"110","subject":"ai_agent","context":"computer_science","predicate":"uses","object":"tool_calling","quantifier":"common","truth_value":"T","evidence_for":["invokes external APIs, search, code execution"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"111","subject":"ai_agent","context":"computer_science","predicate":"uses","object":"planning","quantifier":"universal","truth_value":"T","evidence_for":["decomposes goals into steps"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"112","subject":"ai_agent","context":"computer_science","predicate":"typically_uses","object":"llm","quantifier":"common","truth_value":"T","evidence_for":["LLM as reasoning core"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"113","subject":"transfer_learning","context":"computer_science","predicate":"is_a","object":"training_technique","quantifier":"universal","truth_value":"T","evidence_for":["reuse pretrained model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"114","subject":"transfer_learning","context":"computer_science","predicate":"reduces","object":"training_cost","quantifier":"universal","truth_value":"T","evidence_for":["leverages existing knowledge"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"115","subject":"overfitting","context":"computer_science","predicate":"is_a","object":"training_problem","quantifier":"universal","truth_value":"T","evidence_for":["memorizes training data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"116","subject":"dropout","context":"computer_science","predicate":"prevents","object":"overfitting","quantifier":"partial","truth_value":"T","evidence_for":["randomly deactivates neurons"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"117","subject":"residual_connection","context":"computer_science","predicate":"mitigates","object":"vanishing_gradient","quantifier":"universal","truth_value":"T","evidence_for":["skip connections, ResNet 2015"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"118","subject":"transformer","context":"computer_science","predicate":"uses","object":"residual_connection","quantifier":"universal","truth_value":"T","evidence_for":["around each sublayer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"119","subject":"transformer","context":"computer_science","predicate":"uses","object":"layer_normalization","quantifier":"universal","truth_value":"T","evidence_for":["stabilizes training"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"120","subject":"vae","context":"computer_science","predicate":"is_a","object":"generative_model","quantifier":"universal","truth_value":"T","evidence_for":["variational autoencoder"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"121","subject":"vae","context":"computer_science","predicate":"uses","object":"latent_space","quantifier":"universal","truth_value":"T","evidence_for":["encodes to compressed representation"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"122","subject":"deep_learning","context":"computer_science","predicate":"is_a","object":"machine_learning_subset","quantifier":"universal","truth_value":"T","evidence_for":["multiple layers"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"123","subject":"machine_learning","context":"computer_science","predicate":"is_a","object":"ai_subset","quantifier":"universal","truth_value":"T","evidence_for":["learning from data"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"124","subject":"supervised_learning","context":"computer_science","predicate":"is_a","object":"ml_paradigm","quantifier":"universal","truth_value":"T","evidence_for":["labeled input-output pairs"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"125","subject":"unsupervised_learning","context":"computer_science","predicate":"is_a","object":"ml_paradigm","quantifier":"universal","truth_value":"T","evidence_for":["find patterns without labels"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"126","subject":"reinforcement_learning","context":"computer_science","predicate":"is_a","object":"ml_paradigm","quantifier":"universal","truth_value":"T","evidence_for":["learn via reward signals"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"127","subject":"hallucination","context":"computer_science","predicate":"is_a","object":"llm_failure_mode","quantifier":"universal","truth_value":"T","evidence_for":["generates false but plausible text"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"128","subject":"hallucination","context":"computer_science","predicate":"caused_by","object":"pattern_completion_without_grounding","quantifier":"primary","truth_value":"T","evidence_for":["statistical patterns, no fact verification"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"129","subject":"attention_mechanism","context":"computer_science","predicate":"analogous_to","object":"selective_focus","quantifier":"conceptual","truth_value":"T","evidence_for":["cocktail party problem: focus on relevant signal"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"130","subject":"mamba","context":"computer_science","predicate":"analogous_to","object":"selective_memory","quantifier":"conceptual","truth_value":"T","evidence_for":["choose what to remember/forget from state"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"131","subject":"transformer","context":"computer_science","predicate":"remembers","object":"everything_in_context","quantifier":"universal","truth_value":"T","evidence_for":["full attention over all tokens"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"132","subject":"rnn","context":"computer_science","predicate":"remembers","object":"compressed_history","quantifier":"universal","truth_value":"T","evidence_for":["fixed hidden state"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"133","subject":"transformer","context":"computer_science","predicate":"limited_by","object":"context_window","quantifier":"universal","truth_value":"T","evidence_for":["fixed max sequence length"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"134","subject":"mamba","context":"computer_science","predicate":"not_limited_by","object":"context_window","quantifier":"universal","truth_value":"T","evidence_for":["handles million+ length sequences"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"135","subject":"diffusion_model","context":"computer_science","predicate":"inspired_by","object":"thermodynamics","quantifier":"universal","truth_value":"T","evidence_for":["nonequilibrium statistical physics"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"136","subject":"ssm","context":"computer_science","predicate":"inspired_by","object":"control_theory","quantifier":"universal","truth_value":"T","evidence_for":["continuous-time dynamical systems"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"137","subject":"neural_network","context":"computer_science","predicate":"inspired_by","object":"biological_neurons","quantifier":"universal","truth_value":"T","evidence_for":["original motivation"],"evidence_against":["modern NNs diverge significantly from biology"],"source":"web_search","generation":0},
    {"id":"138","subject":"lora","context":"computer_science","predicate":"is_a","object":"fine_tuning_technique","quantifier":"universal","truth_value":"T","evidence_for":["low-rank adaptation, trains small adapters"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"139","subject":"lora","context":"computer_science","predicate":"reduces","object":"training_cost","quantifier":"universal","truth_value":"T","evidence_for":["trains thousands vs billions of params"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"140","subject":"prompt_engineering","context":"computer_science","predicate":"is_a","object":"inference_technique","quantifier":"universal","truth_value":"T","evidence_for":["crafting inputs to guide model behavior"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"141","subject":"prompt_engineering","context":"computer_science","predicate":"does_not_modify","object":"model_weights","quantifier":"universal","truth_value":"T","evidence_for":["only changes input, not model"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"142","subject":"in_context_learning","context":"computer_science","predicate":"is_a","object":"emergent_capability","quantifier":"universal","truth_value":"T","evidence_for":["learn from examples in prompt without training"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"143","subject":"mamba2","context":"computer_science","predicate":"is_a","object":"mamba_variant","quantifier":"universal","truth_value":"T","evidence_for":["2-8x faster, state space duality framework"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"144","subject":"mamba2","context":"computer_science","predicate":"proves","object":"transformers_are_ssms","quantifier":"theoretical","truth_value":"T","evidence_for":["structured semiseparable matrices connect both"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"145","subject":"attention_mechanism","context":"computer_science","predicate":"is_a","object":"communication_mechanism","quantifier":"universal","truth_value":"T","evidence_for":["tokens share information"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"146","subject":"mlp","context":"computer_science","predicate":"is_a","object":"computation_mechanism","quantifier":"universal","truth_value":"T","evidence_for":["independent per-token processing"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"147","subject":"transformer","context":"computer_science","predicate":"alternates","object":"communication_and_computation","quantifier":"universal","truth_value":"T","evidence_for":["attention (communicate) + FFN (compute) per layer"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"148","subject":"mamba","context":"computer_science","predicate":"merges","object":"communication_and_computation","quantifier":"universal","truth_value":"T","evidence_for":["single block handles both via SSM + projections"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"149","subject":"diffusion_model","context":"computer_science","predicate":"uses","object":"iterative_refinement","quantifier":"universal","truth_value":"T","evidence_for":["many small denoising steps"],"evidence_against":[],"source":"web_search","generation":0},
    {"id":"150","subject":"rag","context":"computer_science","predicate":"combines","object":"retrieval_and_generation","quantifier":"universal","truth_value":"T","evidence_for":["retrieve relevant docs then generate answer"],"evidence_against":[],"source":"web_search","generation":0}
  ]
}
